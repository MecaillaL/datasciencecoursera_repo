-------------- THE NEED FOR OPTIMIZATION ----------------------
Loss function
=>> a loss function to aggregate all the errors into a single measure of the model's predictive performance.(ex. mean squared error)
=> lower lost function means a better model
=> Goal: find the weights that give the lowest value for the loss function.

################################################
--------------- Coding how weight changes affect accuracy----------------

# The data point you will make a prediction for
input_data = np.array([0, 3])

# Sample weights
weights_0 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 1]
            }

# The actual target value, used to calculate the error
target_actual = 3

# Make prediction using original weights
model_output_0 = predict_with_network(input_data, weights_0)

# Calculate error: error_0
error_0 = model_output_0 - target_actual

# Create weights that cause the network to make perfect prediction (3): weights_1
weights_1 = {'node_0': [2, 1],
             'node_1': [1, 2],
             'output': [1, 0]
            }

# Make prediction using new weights: model_output_1
model_output_1 = predict_with_network(input_data, weights_1)

# Calculate error: error_1
error_1 = model_output_1 - target_actual

# Print error_0 and error_1
print(error_0)
print(error_1)

##result:
  <script.py> output:
    6
    0

###################################################
--------------------Scaling up to multiple data points--------------------------------

from sklearn.metrics import mean_squared_error

# Create model_output_0 
model_output_0 = []
# Create model_output_1
model_output_1 = []

# Loop over input_data
for row in input_data:
    # Append prediction to model_output_0
    model_output_0.append(predict_with_network(row, weights_0))
    
    # Append prediction to model_output_1
    model_output_1.append(predict_with_network(row, weights_1))

# Calculate the mean squared error for model_output_0: mse_0
mse_0 = mean_squared_error(target_actuals, model_output_0)

# Calculate the mean squared error for model_output_1: mse_1
mse_1 = mean_squared_error(target_actuals, model_output_1)

# Print mse_0 and mse_1
print("Mean squared error with weights_0: %f" %mse_0)
print("Mean squared error with weights_1: %f" %mse_1)

##result:
  <script.py> output:
    Mean squared error with weights_0: 37.500000
    Mean squared error with weights_1: 49.890625

________________________________________________________________________________

#/# GRADIENT DESCENT
=> If the slope if positive:
  > going opposite the slope means moving to lower numbers.
  > too big a astep might lead as astray.
 Solution: Learning rate
 > update each weight by subtracting learning rate * slope

############################
--------Calculating slopes--------------------------------
# Calculate the predictions: preds
preds = (weights * input_data).sum()

# Calculate the error: error
error = preds - target

# Calculate the slope: slope
slope = 2 * input_data * error

# Print the slope
print(slope)

#########################################
=-------------------Improving model weights-----------------------

# Set the learning rate: learning_rate
learning_rate = 0.01

# Calculate the predictions: preds
preds = (weights * input_data).sum()

# Calculate the error: error
error = preds - target

# Calculate the slope: slope
slope = 2 * input_data * error

# Update the weights: weights_updated
weights_updated = weights - (learning_rate * slope)

# Get updated predictions: preds_updated
preds_updated = (weights_updated * input_data).sum()

# Calculate updated error: error_updated
error_updated = preds_updated - target

# Print the original error
print(error)

# Print the updated error
print(error_updated)

##result:
  <script.py> output:
    7
    5.04
    

############################################
----------------------- Making multiple updates to weights-----------------------------
    
    n_updates = 20
mse_hist = []

# Iterate over the number of updates
for i in range(n_updates):
    # Calculate the slope: slope
    slope = get_slope(input_data, target, weights)
    
    # Update the weights: weights
    weights = weights - 0.01 * slope
    
    # Calculate mse with new weights: mse
    mse = get_mse(input_data, target, weights)
    
    # Append the mse to mse_hist
    mse_hist.append(mse)

# Plot the mse history
plt.plot(mse_hist)
plt.xlabel('Iterations')
plt.ylabel('Mean Squared Error')
plt.show()
    
_________________________________________________________________________________
-----------------BACK PROPAGARATION ------------------------------------------

=>> a technique called “back propagation” to calculate the slopes you need to optimize more complex deep learning models.
=> Just as forward propagation sends input data through the hidden layers and into the output layer, back propagation takes 
    the error from the output layer andpropagates it backward through the hidden layers, towards the input layer.    
PROCESS:
=> Go back one layer at a time
=> Gradients for weights is product of;
    1. Node value feeding into that weight
    2. Slope of loss function w.r.t node it feed into.
    3. Slope of activation function at the node it feeds into
    

 ###########################################
 ------- CREATING A KERAS MODEL ------------
 1. Specify architecture
 2. Compile
 3. Fit the model
 4. USe model for predictions
 
 
----------------------------  Specifying a model    ---------------------------
    
    # Import necessary modules
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

# Save the number of columns in predictors: n_cols
n_cols = predictors.shape[1]

# Set up the model: model
model = Sequential()

# Add the first layer
model.add(Dense(50, activation='relu', input_shape=(n_cols,)))

# Add the second layer
model.add(Dense(32, activation='relu'))

# Add the output layer
model.add(Dense(1))
    
    
########################################################
----------Compiling anad fitting a model --------------
Why you need to compile yoour model?
=> Specify the optimizer, which controls the learning rate.
 The pragmatic approach is to choose a versatile algorithm and use that for most problems. 
 > "Adam" is an excellent choice as your go-to optimizer.
=> Specify loss function

What is a fitting model?
=> applying back propagation and gradient descent with your data to update the weights.
=> scaling data before fitting can ease optimization.

######################################
--------------COMPILE-----------------

# Import necessary modules
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

# Specify the model
n_cols = predictors.shape[1]
model = Sequential()
model.add(Dense(50, activation='relu', input_shape = (n_cols,)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

# Compile the model
model.compile(optimizer = "adam", loss= 'mean_squared_error')

# Verify that model contains information from compiling
print("Loss function: " + model.loss)
    
    
####################################
--------- FITTING THE MODEL---------
#instructions:  Fit the model. Remember that the first argument is 
 the predictive features (predictors), and the data 
 to be predicted (target) is the second argument.
  
# Import necessary modules
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential

# Specify the model
n_cols = predictors.shape[1]
model = Sequential()
model.add(Dense(50, activation='relu', input_shape = (n_cols,)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Fit the model
model.fit(predictors, target)   
   
_______________________________________________________________
--------------CLASSIFICATION MODELS -------------------------------
=> 'categorical cosentropy' loss function
=> similar to log loss: lower is better
=> add metrocs = ['accuracy'] to compile step for easy to understand diagnostics.


# Import necessary modules
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical

# Convert the target to categorical: target
target = to_categorical(df.survived)

# Set up the model
model = Sequential()

# Add the first layer
model.add(Dense(32, activation= 'relu', input_shape= (n_cols,)))

# Add the output layer
model.add(Dense(2, activation= 'softmax'))

# Compile the model
model.compile(optimizer='sgd', loss= 'categorical_crossentropy', metrics= ['accuracy'])

# Fit the model
model.fit(predictors, target)
   
    
####################################
--------------USING MODEL------------
   
   
   # Specify, compile, and fit the model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape = (n_cols,)))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='sgd', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
model.fit(predictors, target)

# Calculate predictions: predictions
predictions = model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:,1]

# Print predicted_prob_true
print(predicted_prob_true)
   
   
   
   
   


