* Machine learning is the process whereby computers learn to make decisions from data without being explicitly programmed.
* Unsupervised learning is the process of uncovering hidden patterns and structures from unlabeled data. 
* Supervised learning is a type of machine learning where the values to be predicted are 
  already known, and a model is built with the aim of accurately predicting values of previously unseen data. 
  Supervised learning uses features to predict the value of a target variable, such as predicting a basketball player's position based on their points per game.


TYPES OF SUPERVISED:
1. CLASSIFICATION
2. REGRESSION

###########################

from sklearn.module import Model
Model = Model()
model.fit(X,y)
model.predict(X_new)


___________________________________________________
----------k-Nearest Neighbors: Fit--------

# Import KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier 

# Create arrays for the features and the target variable
y = churn_df["churn"].values
X = churn_df[["account_length", "customer_service_calls"]].values

# Create a KNN classifier with 6 neighbors
knn = KNeighborsClassifier(6)

# Fit the classifier to the data
knn.fit(X, y)

###############################
-----------------k-Nearest Neighbors: Predict-----------------
# Predict the labels for the X_new
y_pred = knn.predict(X_new)

# Print the predictions for X_new
print("Predictions: {}".format(y_pred)) 

#############################
--------------Train/test split + computing accuracy------------------

# Import the module
from sklearn.model_selection import train_test_split

X = churn_df.drop("churn", axis=1).values
y = churn_df["churn"].values

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42, stratify=y)
knn = KNeighborsClassifier(n_neighbors=5)

# Fit the classifier to the training data
knn.fit(X_train, y_train)

# Print the accuracy
print(knn.score(X_test, y_test))

#############################################
=--------------------Overfitting and underfitting------------------

# Create neighbors
neighbors = np.arange(1, 13)
train_accuracies = {}
test_accuracies = {}

for neighbor in neighbors:
  
  	# Set up a KNN Classifier
  	knn = KNeighborsClassifier(n_neighbors=neighbor)
  
  	# Fit the model
  	knn.fit(X_train, y_train)
  
  	# Compute accuracy
  	train_accuracies[neighbor] = knn.score(X_train, y_train)
  	test_accuracies[neighbor] = knn.score(X_test, y_test)
print(neighbors, '\n', train_accuracies, '\n', test_accuracies)


################################################
----------------------Visualizing model complexity ----------------------------------

# Add a title
plt.title("KNN: Varying Number of Neighbors")

# Plot training accuracies
plt.plot(neighbors, train_accuracies.values(), label="Training Accuracy")

# Plot test accuracies
plt.plot(neighbors, test_accuracies.values(), label="Testing Accuracy")

plt.legend()
plt.xlabel("Number of Neighbors")
plt.ylabel("Accuracy")

# Display the plot
plt.show()


________________________________________________________________________________________________________________________
------------------------------ Introduction to regression ------------------------------------------

** Creating features **

###############################################
import numpy as np

# Create X from the radio column's values
X = sales_df["radio"].values

# Create y from the sales column's values
y = sales_df["sales"].values

# Reshape X
X = X.reshape(-1, 1)

# Check the shape of the features and targets
print(X.shape, y.shape)


################################################
-------Building a linear regression model--------

# Import LinearRegression
from sklearn.linear_model import LinearRegression

# Create the model
reg = LinearRegression()

# Fit the model to the data
reg.fit(X,y)

# Make predictions
predictions = reg.predict(X)

print(predictions[:5])

########################################################
----------- Visualizing a linear regression model--------

# Import matplotlib.pyplot
import matplotlib.pyplot as plt

# Create scatter plot
plt.scatter(X, y, color="blue")

# Create line plot
plt.plot(X, predictions, color="red")
plt.xlabel("Radio Expenditure ($)")
plt.ylabel("Sales ($)")

# Display the plot
plt.show()


#######################################################
-------------- The basics of linear regression ----------------

Fit and predict for regression

# Create X and y arrays
X = sales_df.drop("sales", axis=1).values               <<--#instructions:#Create X, an array containing values of all features in sales_df, and y, containing all values from the "sales" column
y = sales_df["sales"].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Instantiate the model
reg = LinearRegression()

# Fit the model to the data
reg.fit(X_train, y_train)

# Make predictions
y_pred = reg.predict(X_test)
print("Predictions: {}, Actual Values: {}".format(y_pred[:2], y_test[:2]))


#############################################################
---------- Regression performance -------------------------------

# Import mean_squared_error
from sklearn.metrics import mean_squared_error

# Compute R-squared
r_squared = reg.score(X_test, y_test)

# Compute RMSE
rmse = mean_squared_error(y_test, y_pred, squared=False)

# Print the metrics
print("R^2: {}".format(r_squared))
print("RMSE: {}".format(rmse))

##result:
 <script.py> output:
    R^2: 0.9990165886162027
    RMSE: 2942.372219812037


#########################################################
--------- Cross-validation for R-squared ------------------

=>> Cross-validation is a vital approach to evaluating a model. 
It maximizes the amount of data that is available to the model, 
as the model is not only trained but also tested on all of the 
available data. <<=


# Import the necessary modules
from sklearn.model_selection import cross_val_score, KFold

# Create a KFold object
kf = KFold(n_splits=6, shuffle=True, random_state=5)

reg = LinearRegression()

# Compute 6-fold cross-validation scores
cv_scores = cross_val_score(reg, X, y, cv=kf)

# Print scores
print(cv_scores)

#results:
 <script.py> output:
    [0.74451678 0.77241887 0.76842114 0.7410406  0.75170022 0.74406484]

########### ------------- Analyzing cross-validation metrics ------#######

# Print the mean
print(np.mean(cv_results))

# Print the standard deviation
print(np.std(cv_results))

# Print the 95% confidence interval
print(np.quantile(cv_results, [0.025, 0.975]))

#result:
 <script.py> output:
    0.7536937416666666
    0.012305386274436092
    [0.74141863 0.77191915]
   // An average score of 0.75 with a low standard deviation is pretty good for a model out of the box!//
   
   
##############################################################
------------- Regularized regression ---------------------------

=>> regularization in regression, a technique used to avoid overfitting.
Regularization: Penalized large coefficients

types:
1. RIDGE REGRESSION = Ordinary Least Squares loss function +
                      the squared value of each coefficient, multiplied by a constant, alpha.

   alpha = parameter that we need to choose #for which alpha the best model perform
   Hyperparameter = variables used to optimized model parameters.
   

2. LASSO REGRESSION = the OLS loss function +
                      the absolute value of each coefficient multiplied by some constant, alpha.
       => can select important features of a dataset
       => shrinks the coefficients of less important features to zero.
       => features not shrinks to zero are selected by lasso
       
       
##############################################
--------- Regularized regression: Ridge ------

# Import Ridge
from sklearn.linear_model import Ridge
alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]
ridge_scores = []
for alpha in alphas:
  
  # Create a Ridge regression model
  ridge = Ridge(alpha=alpha)
  
  # Fit the data
  ridge.fit(X_train, y_train)
  
  # Obtain R-squared
  score = ridge.score(X_test, y_test)
  ridge_scores.append(score)
print(ridge_scores)

#results:
 <script.py> output:
    [0.9990152104759369, 0.9990152104759373, 0.9990152104759419, 0.9990152104759871, 0.9990152104764387, 0.9990152104809561]
   // The scores don't appear to change much as alpha increases, 
      which is indicative of how well the features explain the variance 
      in the target—even by heavily penalizing large coefficients, 
      underfitting does not occur! //
    
 ################################################################
 ------------- Lasso regression for feature importance ----------

# Import Lasso
from sklearn.linear_model import Lasso

# Instantiate a lasso regression model
lasso = Lasso(alpha= 0.3)
# Fit the model to the data
lasso.fit(X, y)

# Compute and print the coefficients
lasso_coef = lasso.fit(X, y).coef_
print(lasso_coef)
plt.bar(sales_columns, lasso_coef)
plt.xticks(rotation=45)
plt.show()

#results:
 <script.py> output:
    [ 3.56256962 -0.00397035  0.00496385]   //it clear that expenditure on TV advertising is the most important feature in the dataset to predict sales values!//
    
    
_____________________________________________________________________________________________________________________________________
------------------------------ FINE-TUNING YOUR MODEL ------------------------------------
--------------------  HOW GOOD IS YOUR MDOEL? ----------------------------------

** CLASSIFICATION METRICS

CLASS IMBALANCE => The situation where one class is more frequent is called class imbalance.

*Confusion Matrix for assesing classification performance
   = Accuracy > TP + TN / TP+TN+FP+FN
   = Precisiom > TP / TP+FP
   = Sensitivity/Recall > TP/ TP+FN
   = F1 score > 2* [(PRECISION*RECALL) / (PRECISION + RECALL)]


###################################################
----- Assessing a diabetes prediction classifier

  # Import confusion matrix
from sklearn.metrics import classification_report, confusion_matrix

knn = KNeighborsClassifier(n_neighbors=6)

# Fit the model to the training data
knn.fit(X_train, y_train)

# Predict the labels of the test data: y_pred
y_pred = knn.predict(X_test)

# Generate the confusion matrix and classification report
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))  
    

####################################################################
--------------- Logistic regression and the ROC curve ----------------

LOGISTIC REGRESSION
==>> is used for classification. This model calculates the probability, p, that an observation belongs to a binary class.


# Import LogisticRegression
from sklearn.linear_model import LogisticRegression

# Instantiate the model
logreg = LogisticRegression()

# Fit the model
logreg.fit(X_train, y_train)

# Predict probabilities
y_pred_probs = logreg.predict_proba(X_test)[:, 1]

print(y_pred_probs[:10])


###############################################################
--------------- ROC CURVE -------
=> ROC curve to visualize how the true positive rate and false positive rate vary as the decision threshold changes.


# Import roc_curve
from sklearn.metrics import roc_curve

# Generate ROC curve values: fpr, tpr, thresholds
fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)

plt.plot([0, 1], [0, 1], 'k--')

# Plot tpr against fpr 
plt.plot(fpr, tpr)       #== fpr(false positive rate), tpr(true positive rate)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Diabetes Prediction')
plt.show()


##############################

# Import roc_auc_score
from sklearn.metrics import roc_auc_score

# Calculate roc_auc_score
print(roc_auc_score(y_test, y_pred_probs))

# Calculate the confusion matrix
print(confusion_matrix(y_test, y_pred))

# Calculate the classification report
print(classification_report(y_test, y_pred))


##################################################################################
---------------HYPERPARAMETER TUNING ---------------------------------
==>> Parameters that we specify before fitting a model, like alpha and n_neighbors, are called hyperparameters

###########################
------------Hyperparameter tuning with GridSearchCV ----------


# Import GridSearchCV
from sklearn.model_selection import GridSearchCV

# Set up the parameter grid
param_grid = {"alpha": np.linspace(0.00001 , 1, 20)}   #>Set up a parameter grid for "alpha", using np.linspace() to create 20 evenly spaced values ranging from 0.00001 to 1.

# Instantiate lasso_cv
lasso_cv = GridSearchCV(lasso, param_grid, cv=kf)

# Fit to the training data
lasso_cv.fit(X_train, y_train)
print("Tuned lasso paramaters: {}".format(lasso_cv.best_params_))
print("Tuned lasso score: {}".format(lasso_cv.best_score_))

##result:
  <script.py> output:
    Tuned lasso paramaters: {'alpha': 1e-05}
    Tuned lasso score: 0.3307880723812198


#########################
---------Hyperparameter tuning with RandomizedSearchCV---------

# Create the parameter space
params = {"penalty": ["l1", "l2"],
         "tol": np.linspace(0.0001, 1.0, 50),
         "C": np.linspace(0.1, 1.0, 50),
         "class_weight": ["balanced", {0:0.8, 1:0.2}]}

# Instantiate the RandomizedSearchCV object
logreg_cv = RandomizedSearchCV(logreg, params, cv=kf)

# Fit the data to the model
logreg_cv.fit(X_train, y_train)

# Print the tuned parameters and score
print("Tuned Logistic Regression Parameters: {}".format(logreg_cv.best_params_))
print("Tuned Logistic Regression Best Accuracy Score: {}".format(logreg_cv.best_score_))

##RESULT:
  <script.py> output:
    Tuned Logistic Regression Parameters: {'tol': 0.14294285714285712, 'penalty': 'l2', 'class_weight': 'balanced', 'C': 0.6326530612244898}
    Tuned Logistic Regression Best Accuracy Score: 0.7460082633613221


_____________________________________________________________________________________________________
------------ PREPROCESSING DATA -----------------------------------------------------------------


#######################################
------ Creating dummy variables-------

# Create music_dummies
music_dummies = pd.get_dummies(music_df, drop_first=True)

# Print the new DataFrame's shape
print("Shape of music_dummies: {}".format(music_dummies.shape))


#####################################
---- Regression with categorical features --------

# Create X and y
X = music_dummies.drop("popularity", axis=1)
y = music_dummies['popularity']

# Instantiate a ridge model
ridge = Ridge(alpha= 0.2)

# Perform cross-validation
scores = cross_val_score(ridge, X, y, cv=kf, scoring="neg_mean_squared_error")

# Calculate RMSE
rmse = np.sqrt(-scores)
print("Average RMSE: {}".format(np.mean(rmse)))
print("Standard Deviation of the target array: {}".format(np.std(y)))


##result:
  <script.py> output:
    Average RMSE: 8.236853840202297
    Standard Deviation of the target array: 14.02156909907019
  // An average RMSE of approximately 8.24 is lower than the standard 
  deviation of the target variable (song popularity), suggesting the model is reasonably accurate.//
  

_____##############################__________________
--------- HANDLING MISSING DATA ----------------------

# Print missing values for each column
print(music_df.isna().sum().sort_values())

# Remove values where less than 5% are missing
music_df = music_df.dropna(subset=["genre", "popularity", "loudness", "liveness", "tempo"])

# Convert genre to a binary feature
music_df["genre"] = np.where(music_df["genre"] == "Rock", 1, 0)

print(music_df.isna().sum().sort_values())
print("Shape of the `music_df`: {}".format(music_df.shape))


##########################################
---- Pipeline for song genre prediction: I -----

# Import modules
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Instantiate an imputer
imputer = SimpleImputer()

# Instantiate a knn model
knn = KNeighborsClassifier(n_neighbors =3)

# Build steps for the pipeline
steps = [("imputer", imputer), 
         ("knn", knn)]

##########################################
------ Pipeline for song genre prediction: II =--------

steps = [("imputer", imp_mean),
        ("knn", knn)]

# Create the pipeline
pipeline = Pipeline(steps)

# Fit the pipeline to the training data
pipeline.fit(X_train, y_train)
# Make predictions on the test set
y_pred = pipeline.predict(X_test)

# Print the confusion matrix
print(confusion_matrix(y_test, y_pred))

##result:
  <script.py> output:
    [[79  9]
     [ 4 82]]
     
     
#################################################################
------------ CENTERING AND SCALING -----------------------------

Why scale our data?
     => Many machine learning models use some form of distance to inform them, 
     so if we have features on far larger scales, they can disproportionately 
     influence our model. For example, KNN uses distance explicitly when making 
     predictions. For this reason, we actually want features to be on a similar 
     scale. To achieve this, we can normalize or standardize our data, often 
     referred to as scaling and centering.
     
  ###############################
  ----- Centering and scaling for regression-----------
  
     
 # Import StandardScaler
from sklearn.preprocessing import StandardScaler

# Create pipeline steps
steps = [("scaler", StandardScaler()),
         ("lasso", Lasso(alpha=0.5))]

# Instantiate the pipeline
pipeline = Pipeline(steps)
pipeline.fit(X_train, y_train)

# Calculate and print R-squared
print(pipeline.score(X_test, y_test))    
     
 ##result:
  <script.py> output:
    0.6193523316282489
     
       ###############################
 --------------Centering and scaling for classification--------    
 
 # Build the steps
steps = [("scaler", StandardScaler()),
         ("logreg", LogisticRegression())]
pipeline = Pipeline(steps)

# Create the parameter space
parameters = {"logreg__C": np.linspace(0.001, 1.0, 20)}
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
                                                    random_state=21)

# Instantiate the grid search object
cv = GridSearchCV(pipeline, param_grid=parameters)

# Fit to the training data
cv.fit(X_train, y_train)
print(cv.best_score_, "\n", cv.best_params_)
 
 ##result:
  <script.py> output:
    0.8425 
     {'logreg__C': 0.1061578947368421}
 //Using a pipeline shows that a logistic regression model with "C" set to approximately 0.1 produces a model with 0.8425 accuracy!//
 
 
 
 ###########################################
 --- Visualizing regression model performance -----------
 
 models = {"Linear Regression": LinearRegression(), "Ridge": Ridge(alpha=0.1), "Lasso": Lasso(alpha=0.1)}
results = []

# Loop through the models' values
for model in models.values():
  kf = KFold(n_splits=6, random_state=42, shuffle=True)
  
  # Perform cross-validation
  cv_scores = cross_val_score(model, X_train, y_train, cv=kf)
  
  # Append the results
  results.append(cv_scores)
  
# Create a box plot of the results
plt.boxplot(results, labels=models.keys())
plt.show()
 
 
 ###########################################
 ------ Predicting on the test set -----------
 
 # Import mean_squared_error
from sklearn.metrics import mean_squared_error

for name, model in models.items():
  
  # Fit the model to the training data
  model.fit(X_train_scaled, y_train)
  
  # Make predictions on the test set
  y_pred = model.predict(X_test_scaled)
  
  # Calculate the test_rmse
  test_rmse = mean_squared_error(y_test, y_pred, squared=False)
  print("{} Test Set RMSE: {}".format(name, test_rmse))
 
 #result:
  <script.py> output:
    Linear Regression Test Set RMSE: 0.1198885150594757
    Ridge Test Set RMSE: 0.11987066103299669
 //The linear regression model just edges the best performance, although the difference is a RMSE of 0.00001 for popularity!//
 
 
 ##############################################
 --Visualizing classification model performance----
 
 # Create models dictionary
models = {"Logistic Regression": LogisticRegression(), "KNN": KNeighborsClassifier(), "Decision Tree Classifier": DecisionTreeClassifier()}
results = []

# Loop through the models' values
for model in models.values():
  
  # Instantiate a KFold object
  kf = KFold(n_splits=6, random_state=12, shuffle=True)
  
  # Perform cross-validation
  cv_results = cross_val_score(model, X_train_scaled, y_train, cv=kf)
  results.append(cv_results)
plt.boxplot(results, labels=models.keys())
plt.show()
 
 
 ############################################
 ------- Pipeline for predicting song popularity-----------
 
 # Create steps
steps = [("imp_mean", SimpleImputer()), 
         ("scaler", StandardScaler()), 
         ("logreg", LogisticRegression())]

# Set up pipeline
pipeline = Pipeline(steps)
params = {"logreg__solver": ["newton-cg", "saga", "lbfgs"],
         "logreg__C": np.linspace(0.001, 1.0, 10)}

# Create the GridSearchCV object
tuning = GridSearchCV(pipeline, param_grid=params)
tuning.fit(X_train, y_train)
y_pred = tuning.predict(X_test)

# Compute and print performance
print("Tuned Logistic Regression Parameters: {}, Accuracy: {}".format(tuning.best_params_, tuning.score(X_test, y_test)))
 
 #result:
  <script.py> output:
    Tuned Logistic Regression Parameters: {'logreg__C': 0.112, 'logreg__solver': 'newton-cg'}, Accuracy: 0.82
 
 
 
 
 -END
 
     
