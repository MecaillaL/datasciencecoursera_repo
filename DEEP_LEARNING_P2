----INTRODUCTION-------

NEURAL NETWORK
=> more nodes in the hidden layer, and therefore, greater ability to capture interactions.


FORWARD PROPAGATION
=> multiply-add process
=> dot product
=> forward propagation for one data point at a time
=> output is the prediction for that data point

###################################
-------------Coding the forward propagation algorithm---------------------

# Calculate node 0 value: node_0_value
node_0_value = (input_data * weights['node_0']).sum()

# Calculate node 1 value: node_1_value
node_1_value = (input_data * weights['node_1']).sum()

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_value, node_1_value])

# Calculate output: output
output = (hidden_layer_outputs * weights['output']).sum()

# Print output
print(output)

##result:
  <script.py> output:
    -39     //network generated a prediction of -39//


######################################
------------------------ Activation Function---------------------------------------------------------
==>> An activation function allows the model to capture non-linearities.
=> applied to node inputs to produce node outputs

1. Rectified Linear Activation function
  =>is a function applied at each node. It converts the node's input into some output.

###########Fill in the definition of the relu() function:
def relu(input):
    '''Define your relu activation function here'''
    # Calculate the value for the output of the relu function: output
    output = max(0, input)
    
    # Return the value just calculated
    return(output)

# Calculate node 0 value: node_0_output
node_0_input = (input_data * weights['node_0']).sum()
node_0_output = relu(node_0_input)

# Calculate node 1 value: node_1_output
node_1_input = (input_data * weights['node_1']).sum()
node_1_output = relu(node_1_input)

# Put node values into array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_output, node_1_output])

# Calculate model output (do not apply relu)
model_output = (hidden_layer_outputs * weights['output']).sum()

# Print model output
print(model_output)

##result:
  <script.py> output:
    52     //You predicted 52 transactions. Without this activation function, 
             you would have predicted a negative number! The real power of 
             activation functions will come soon when you start tuning model weights. //

##########################################################
-------- Applying the network to many observations/rows of data----------------------------


# Define predict_with_network()
def predict_with_network(input_data_row, weights):

    # Calculate node 0 value
    node_0_input = (input_data_row * weights['node_0']).sum()
    node_0_output = relu(node_0_input)

    # Calculate node 1 value
    node_1_input = (input_data_row * weights['node_1']).sum()
    node_1_output = relu(node_1_input)

    # Put node values into array: hidden_layer_outputs
    hidden_layer_outputs = np.array([node_0_output, node_1_output])
    
    # Calculate model output
    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()
    model_output = relu(input_to_final_layer)
    
    # Return model output
    return(model_output)

# Create empty list to store prediction results
results = []
for input_data_row in input_data:
    # Append prediction to results
    results.append(predict_with_network(input_data_row, weights))

# Print results
print(results)    

##results: 
  <script.py> output:
    [52, 63, 0, 148]


##########################################################
-------- DEEPER NETWORKS---------------------------

REPRESENTATION LEARNING
=> Deep networks internally build representations of pattern in data.
=> partially replace the need for feature engineering.
=> subsequent layers build increasingly sophisticated representations 
   of the raw data, until we get to a stage where we can make predictions.



###############################
--------------------MULTI-LAYER NEURAL NETWORKS---------------

def predict_with_network(input_data):
    # Calculate node 0 in the first hidden layer
    node_0_0_input = (input_data * weights['node_0_0']).sum()
    node_0_0_output = relu(node_0_0_input)

    # Calculate node 1 in the first hidden layer
    node_0_1_input = (input_data * weights['node_0_1']).sum()
    node_0_1_output = relu(node_0_1_input)

    # Put node values into array: hidden_0_outputs
    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])
    
    # Calculate node 0 in the second hidden layer
    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()
    node_1_0_output = relu(node_1_0_input)

    # Calculate node 1 in the second hidden layer
    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()
    node_1_1_output = relu(node_1_1_input)

    # Put node values into array: hidden_1_outputs
    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])

    # Calculate model output: model_output
    model_output = (hidden_1_outputs* weights['output']).sum()
    
    # Return model_output
    return(model_output)

output = predict_with_network(input_data)
print(output)

##result:
  <script.py> output:
    182


--next chapt
